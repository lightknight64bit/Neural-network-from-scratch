{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Training an AND gate"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "This notebook aims to train a neural network from scratch, we will be training a very simple AND gate.  \n",
    "The only external library used here is numpy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We will be creating the following neural network\n",
    "<img src = \"https://raw.githubusercontent.com/lightknight64bit/Neural-network-from-scratch/ac0680cc7472b192dbc5c6dbd636c9b6f2193263/pic.svg\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "As we can see the neural network has 1 input layer with 2 neurons, 1 hidden layer with 5 neurons and 1 output layer with only one neuron.   \n",
    "\n",
    "The size of input layer and output layer is based on the number of inputs given and output format which in this case is 2 and 1 respectively(We are giving 2 bits as input and expecting an AND output of the two). The size of the hidden layer can be whatever we wish and is 5 for the scope of this notebook"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Let us now look at the equations required to train the neural network."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Activation function\n",
    "\n",
    "We are using the sigmoid activation function which is given by:-    \n",
    "## $f(x) = \\frac{1}{1 + exp(-x)}$  \n",
    "let us denote this by\n",
    "## $\\sigma(x)$\n",
    "The derivative of this function is \n",
    "## $\\frac{exp(-x)}{1+exp(-x)} = (\\sigma(x))(1-\\sigma(x))$\n",
    "\n",
    "## Feed forward\n",
    "Now let the weights for the hidden layer be $W_{1}$ with bias $B_{1}$ and the weights for the output layer be $W_{2}$ with bias $B_{2}$\n",
    "Size for $W_{1}$ will be $m$X$n$ where m is the no. of columns in the input matrix and n is the number of neurons. We have an input size of $4$X$2$ since we have only 4 combinations of $0s$ and $1s$ and only 2 bits.Our input matrix X looks like this:-\n",
    "\n",
    "$X =  \n",
    "\\begin{pmatrix}\n",
    "0 & 0 \\\\\n",
    "0 & 1 \\\\\n",
    "1 & 0 \\\\\n",
    "1 & 1\n",
    "\\end{pmatrix}\n",
    "$ \n",
    " \n",
    "Therefore the size of $W_1$ should be $2$X$5$ as input as 2 columns and we are using 5 neurons.  \n",
    "  \n",
    "The bias size should be $1$X$5$(One bias per neuron)\n",
    "\n",
    "Similiarly we can infer that the weights $W_{2}$ for output layer should be $5$X$1$ and bias $B_{2}$ should be $1$X$1$  \n",
    "Now when we give the inputs to the hidden layer, we get a new matrix $z$ which is given by the equation \n",
    "### $z = \\sigma(XW_{1} + B_{1})$\n",
    "\n",
    "The output $y$ is then given by \n",
    "### $y = \\sigma(ZW_{2} + B_{2})$\n",
    "\n",
    "In this notebook we will initalize our weight and bias matrices by $1$\n",
    "\n",
    "## Backpropogation  \n",
    "Backpropogation is the crux of training neural networks. We will use gradient descent to estimate weights. This notebook does not provide an explanation for gradient descent, however if you don't know then refer this article [Introduction to gradient descent.](https://montjoile.medium.com/an-introduction-to-gradient-descent-algorithm-34cf3cee752b)\n",
    "\n",
    "Let the error function denoted by $E$ be \n",
    "### $E = \\frac{1}{2}\\Sigma{(y - Y)^2}$ \n",
    "\n",
    "where $y$ is our output and Y is our ground truth matrix which is\n",
    "$Y = \n",
    "\\begin{pmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "1\n",
    "\\end{pmatrix}\n",
    "$\n",
    "We will measure the Derivative of the Error with respect to the weights and biases and subtract the weights and bias matrices with their respective derivatives times the learning rate. For example :- \n",
    "Let the derivative of $E$ with respect to $W_{2}$ be \n",
    "### $\\frac{dE}{dW_{2}}$\n",
    "\n",
    "Then we will subtract learning rate times $\\frac{dE}{dW_{2}}$ from $W_{2}$ i.e :-\n",
    "### $W_{2} = W_{2} - \\alpha*\\frac{dE}{dW_{2}}$ \n",
    "where $\\alpha$ is our learning rate\n",
    "\n",
    "The equations for derivatives and bias are given by:-\n",
    "### $\\frac{dE}{dW_{2}} = Z^T*\\delta_{2}$\n",
    "### $\\frac{dE}{dW_{1}} = X^T*\\delta_{1}$\n",
    "### $\\frac{dE}{dB_{2}} = \\delta_{2}$\n",
    "### $\\frac{dE}{dB_{1}} = \\delta_{1}$\n",
    "where \n",
    "### $\\delta_{2} = (y-Y) \\odot \\sigma' (ZW_{2} + B_{2})$\n",
    "and \n",
    "### $\\delta_{1} = \\delta_{2}W_{2}^T \\odot \\sigma' (XW_{1} + B_{1})$\n",
    "$\\odot$ represents hadarmard product of matrices.\n",
    "For the bias matrices replace columns of the respective $\\delta$ matrix with their row sums so it matches with the size.\n",
    "\n",
    "For the derivation of these equations refer this article :- [Proof](https://sudeepraja.github.io/Neural/)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Python implementation using numpy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "source": [
    "Matrix definitions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0],[0,1], [1,0], [1,1]]).reshape(4,2)\n",
    "Y = np.array([0, 0, 0, 1]).reshape(4,1)\n",
    "w1 = np.ones([2, 5])\n",
    "b1 = np.ones([1,5])\n",
    "w2 = np.ones([5,1])\n",
    "b2 = np.array([1])"
   ]
  },
  {
   "source": [
    "Activation function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# activation function\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def sigmoid_der(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 4,
   "outputs": []
  },
  {
   "source": [
    "## Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "epochs = 40000\n",
    "for i in range(epochs):\n",
    "    #Feed forward\n",
    "    z = np.dot(X, w1) + b1\n",
    "    z = sigmoid(z)\n",
    "    y = np.dot(z, w2) + b2\n",
    "    y = sigmoid(y)\n",
    "    #backpropogation\n",
    "    lr = 0.3\n",
    "    E = y-Y\n",
    "    d2 = np.multiply(E, sigmoid_der(np.dot(z, w2) + b2))\n",
    "    d1 = np.multiply(np.dot(d2, w2.T), sigmoid_der(np.dot(X, w1) + b1))\n",
    "    w2_err = np.dot(z.T, d2)\n",
    "    w1_err = np.dot(X.T, d1)\n",
    "    b2_err = d2\n",
    "    b1_err = d1\n",
    "\n",
    "    w2 -= lr* w2_err\n",
    "    w1 -= lr*w1_err\n",
    "    b2 = b2-lr*np.sum(b2_err)\n",
    "    b1 = b1-lr*np.sum(b1_err, axis=0)\n",
    "\n",
    "\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 5,
   "outputs": []
  },
  {
   "source": [
    "## Prediction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.99223727]])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# The network has been trained, let's check out the output\n",
    "x = [1,1]\n",
    "z = sigmoid(np.dot(x, w1) + b1)\n",
    "y = sigmoid(np.dot(z, w2)+ b2)\n",
    "y"
   ]
  },
  {
   "source": [
    "We see that the output is very close to 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}